{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avito ads classification\n",
    "\n",
    "Denisov Ilia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import keras\n",
    "# from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "import pymorphy2\n",
    "from sklearn.utils import shuffle\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from tqdm import tqdm_notebook, tqdm, tqdm_gui\n",
    "from keras.models import load_model\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataframes\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "categories = pd.read_csv('data/category.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of titles\n",
      "count    489517.000000\n",
      "mean          3.826913\n",
      "std           1.974856\n",
      "min           1.000000\n",
      "25%           2.000000\n",
      "50%           4.000000\n",
      "75%           5.000000\n",
      "max          19.000000\n",
      "dtype: float64\n",
      "Lengths of descriptions\n",
      "count    489517.000000\n",
      "mean         38.276783\n",
      "std          50.858614\n",
      "min           1.000000\n",
      "25%          10.000000\n",
      "50%          20.000000\n",
      "75%          44.000000\n",
      "max         646.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Lengths of titles\")\n",
    "print(pd.Series([len(i.split()) for i in train_df.title]).describe())\n",
    "print(\"Lengths of descriptions\")\n",
    "print(pd.Series([len(i.split()) for i in train_df.description]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenization and every word to its normal form \n",
    "split_into_tokens = lambda text : re.sub(\"[^\\w]\", \" \",  text).split()\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def normalize(text):\n",
    "    normal_words = [morph.parse(word)[0].normal_form for word in split_into_tokens(text)]\n",
    "    return normal_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 243166/243166 [29:00<00:00, 145.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 243166/243166 [02:04<00:00, 1949.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# normalize and save descriptions\n",
    "# normal_descrs = []\n",
    "# for i in tqdm(train_df.description):\n",
    "#     normal_descrs.append(normalize(i))\n",
    "#pickle.dump(normal_descrs, open('dump/normalized_descrs.pckl', 'wb'))\n",
    "\n",
    "# normalize and save titles\n",
    "# normal_titles = []\n",
    "# for i in tqdm(train_df.title):\n",
    "#     normal_titles.append(normalize(i))\n",
    "# pickle.dump(normal_titles, open('dump/normalized_titles.pckl', 'wb'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load, convert array of text arrays to array of strings and concat(titles, desctiptions)\n",
    "normal_descrs = pickle.load(open('dump/normalized_descrs.pckl', 'rb'))\n",
    "normal_titles = pickle.load(open('dump/normalized_titles.pckl', 'rb'))\n",
    "text_arrays = [i+j for i, j in zip(normal_titles, normal_descrs)]\n",
    "texts = [' '.join(i) for i in text_arrays]\n",
    "del normal_descrs, normal_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# u can load fitted tokenizer\n",
    "tk = pickle.load(open('dump/tk_v3.pckl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer.\n",
      "Tokenizer was fitted.\n"
     ]
    }
   ],
   "source": [
    "# enum every word in collection\n",
    "nb_words = 150000\n",
    "tk = Tokenizer(num_words=nb_words)\n",
    "print('Fitting tokenizer.')\n",
    "tk.fit_on_texts(texts)\n",
    "# dump tokenizer\n",
    "pickle.dump(tk, open('dump/tk_v3.pckl', 'wb'))\n",
    "print('Tokenizer was fitted.')\n",
    "# encode every word in texts with a number\n",
    "X = tk.texts_to_sequences(texts)\n",
    "# create one hot arrays for fitting\n",
    "Y = np.array(pd.get_dummies(train_df.category_id))\n",
    "# pad sequences to the same length for NN\n",
    "max_descr_length = 620\n",
    "X = sequence.pad_sequences(X, maxlen=max_descr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabular size - 341824. Using 150000 most frequent words.\n"
     ]
    }
   ],
   "source": [
    "print('Vocabular size - {}. Using {} most frequent words.'.format(len(tk.word_counts), nb_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split of train sample for train and validation samples\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fitting took about 6 hours on NVIDIA GTX1060 (1 hour/epoch)\n",
    "# load fitted model\n",
    "model = load_model('dump/model_v3.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 620, 32)           4800000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 54)                6966      \n",
      "=================================================================\n",
      "Total params: 4,889,398\n",
      "Trainable params: 4,889,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4412a708cd450d84674a682ae30ab8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa42fbb27b344b78d419649963d875c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371968/|/[loss: 2.139, acc: 0.419] 100%|| 371968/372032 [58:30<00:00, 108.00it/s]                                      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5184f72cf5b426596b3c4e5f482e611"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371968/|/[loss: 0.560, acc: 0.844] 100%|| 371968/372032 [58:07<00:00, 107.05it/s]                                      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1551d1941c4da584d4254cb3fb9bab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371968/|/[loss: 0.374, acc: 0.889] 100%|| 371968/372032 [58:07<00:00, 107.35it/s]                                      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0845da027c5949dfb863fa18890dc849"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371968/|/[loss: 0.298, acc: 0.909] 100%|| 371968/372032 [58:08<00:00, 107.41it/s]                                      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2660f79eee415f8842fccb7cdcd25c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371968/|/[loss: 0.245, acc: 0.924] 100%|| 371968/372032 [58:01<00:00, 106.01it/s]                                      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59671036d774ebab4ac7f0d43d01ef1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371968/|/[loss: 0.203, acc: 0.936] 100%|| 371968/372032 [59:23<00:00, 104.85it/s]                                      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2992b587e10>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting of NN\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embedding_vecor_length, input_length=max_descr_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(set(train_df.category_id)), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_split=0.05, shuffle=True, epochs=6, batch_size=64, verbose=0, callbacks=[TQDMNotebookCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('dump/model_v2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy on 54 categories is 0.88\n"
     ]
    }
   ],
   "source": [
    "y_prob = model.predict(x_test)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "y_test_categories = np.argmax(y_test, axis=1)\n",
    "print('Validation accuracy on 54 categories is {:0.2f}'.format(accuracy_score(y_test_categories, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>depth</th>\n",
       "      <th>lvl0_label</th>\n",
       "      <th>lvl1_label</th>\n",
       "      <th>lvl2_label</th>\n",
       "      <th>lvl3_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_id  depth  lvl0_label  lvl1_label  lvl2_label  lvl3_label\n",
       "0            0      3           0          19           4           0\n",
       "1            1      2           0          11           0           0\n",
       "2            2      3           0          19           2           0\n",
       "3            3      3           0          14          28           0\n",
       "4            4      3           0           4          19           0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load new categories for different levels, look at \"categories_transform.ipynb\"\n",
    "cats = pd.read_csv('new_categories.csv')\n",
    "cats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make category_id to lvl{}_label conversion\n",
    "category2lvl0_label = dict(list(zip(cats.category_id, cats.lvl0_label)))\n",
    "category2lvl1_label = dict(list(zip(cats.category_id, cats.lvl1_label)))\n",
    "category2lvl2_label = dict(list(zip(cats.category_id, cats.lvl2_label)))\n",
    "category2lvl3_label = dict(list(zip(cats.category_id, cats.lvl3_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for lvl 0 categories is 0.96\n",
      "Accuracy for lvl 1 categories is 0.94\n",
      "Accuracy for lvl 2 categories is 0.89\n",
      "Accuracy for lvl 3 categories is 0.98\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate([category2lvl0_label, category2lvl1_label, category2lvl2_label, category2lvl3_label]):\n",
    "    y_pred0 = np.vectorize(d.get)(y_pred)\n",
    "    y_test0 = np.vectorize(d.get)(y_test_categories)\n",
    "    acc = accuracy_score(y_test0, y_pred0)\n",
    "    print('Accuracy for lvl {} categories is {:0.2f}'.format(i, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize and save test descriptions\n",
    "# normal_test_descrs = []\n",
    "# for i in tqdm(test_df.description):\n",
    "#     normal_test_descrs.append(normalize(i))\n",
    "# pickle.dump(normal_test_descrs, open('dump/normalized_test_descrs.pckl', 'wb'))\n",
    "\n",
    "# normalize and save test titles\n",
    "# normal_test_titles = []\n",
    "# for i in tqdm(test_df.title):\n",
    "#     normal_test_titles.append(normalize(i))\n",
    "# pickle.dump(normal_test_titles, open('dump/normalized_test_titles.pckl', 'wb'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_test_descrs = pickle.load(open('dump/normalized_test_descrs.pckl', 'rb'))\n",
    "normal_test_titles = pickle.load(open('dump/normalized_test_titles.pckl', 'rb'))\n",
    "test_text_arrays = [i+j for i, j in zip(normal_test_titles, normal_test_descrs)]\n",
    "test_texts = [' '.join(i) for i in test_text_arrays]\n",
    "del normal_test_descrs, normal_test_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = tk.texts_to_sequences(test_texts)\n",
    "# pad sequences to the same length for NN\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_descr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_categories_probs = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_categories = np.argmax(test_categories_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = test_df.assign(category_id=test_categories)\n",
    "test_df = test_df.assign(category=np.array(categories.loc[test_categories].name))\n",
    "test_df.to_csv('answer/answer_extended.csv')\n",
    "test_df[['item_id', 'category_id']].to_csv('answer/answer.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
